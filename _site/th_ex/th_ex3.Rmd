---
title: "Take-home Exercise 3"
description: |
  In this take-home exercise 3, we will examine the financial status of the city of Engagement, Ohio USA using tidyverse, ggoplot2 and its extension.
author:
  - name: Hulwana Saifulzaman 
    url: https://www.linkedin.com/in/hulwana-saifulzaman/
    affiliation: SMU, Master of IT in Business
    affiliation_url: https://scis.smu.edu.sg/master-it-business
date: "`r Sys.Date()`"
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      eval = TRUE,
                      warning = FALSE,
                      message = FALSE)
```

# 1. Overview

In this take-home exercise, appropriate static statistical graphics methods are used to reveal the financial status of the city of Engagement, Ohio USA, particularly on the business aspect. 

The key questions to address are:

● Which businesses appear to be more prosperous?

● Which appear to be struggling?

Due to limited data available, we will only examine the businesses of restaurants and pubs of the city of Engagement.

The data would be processed by using appropriate tidyverse family of packages and the statistical graphics would be prepared using ggplot2 and its extensions.


# 2. Getting Started
Before we get started, it is important for us to ensure that the required R packages have been installed. If yes, we will load the R packages. If they have yet to be installed, we will install the R packages and load them onto R environment.

The chunk code below will do the trick.

```{r}
packages = c('tidyverse', 'lubridate', 'patchwork', 'RColorBrewer', 'ggthemes')
for(p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p, character.only = T)
}
```

# 3. Dataset

The *TravelJournal* dataset has been obtained from the *Journals* folder retrieved from [**Vast Challenge 2022**](https://vast-challenge.github.io/2022/) website.

The reason why this dataset is used as opposed to *FinancialJournal.csv* is because it contains data of the *travelEndLocationId* which is crucial in identifying how mcuh sales were genrated by each pubs or restaurants.

## 3.1. Importing Data

The code chunk below import *TravelJournal.csv* from the data folder by using [`read_csv()`](https://readr.tidyverse.org/reference/read_delim.html) of [**readr**](https://readr.tidyverse.org/) into R and save it as an tibble data frame called *travel*.

```{r}
travel <- read_csv("data/TravelJournal.csv")
```

## 3.2. Data Definition

The following data definition has been extracted from the *VAST Challenge 2022 Dataset Descriptions* file which can be obtained from the *Vast-Challenge-2022* folder downloaded earlier.

### Data definition

*TravelJournal.csv* data contains information about about participants’ motivation for movement around the city. This provides a compressed summary and additional context regarding location-event and financial transaction information contained in the Participant Logs.

● participantId (integer): unique ID corresponding to the participant in question

● travelStartTime (datetime): the time when the participant started traveling

● travelStartLocationId (integer): the unique ID corresponding to the location
the participant is leaving when they begin to travel, NA if unknown

● travelEndTime (datetime): the time when the participant concluded their travel

● travelEndLocationId (integer): the unique ID corresponding to the location the
participant is traveling to

● purpose (string factor): a description of the purpose for the recorded travel, one of:
{“Coming Back From Restaurant”, “Eating”, “Going Back to Home”, “Recreation (Social
Gathering)”, “Work/Home Commute”}

● checkInTime (datetime): the time when the participant checked in to their
destination

● checkOutTime (datetime): the time when the participant left their destination

● startingBalance (double): the participant’s starting balance at the beginning
of their travels

● endingBalance (double): the participant’s ending balance at the conclusion of
their travel

## 3.3. Data Structure and Summary

The following codes are executed to reveal the structure and summary statistics of the data:

```{r}
str(travel)
```

## 3.4. Data Preparation

In order to get the sales volume for each of the pubs and restaurants, we will need to execute the following steps:

● Filter the purpose to consists only of "Recreation (Social Gathering)" and "Eating".
"Recreation (Social Gathering)" would refer to pubs whereas "Eating" would refer to restaurants.

● Compute the amount spent by a customer at each venue by subtracting the endingBalance from the startingBalance.

● Extract year-month, weekday and hour from the *checkInTime*. This will be used in the plotting of line chart (to see the monthly sales pattern) as well as for the generation of heatmap to visualize the check in rate of customers across the weekdays at an hourly interval.

```{r}
sales <- travel %>% select(participantId, travelEndLocationId, purpose, checkInTime, startingBalance, endingBalance) %>% filter(purpose == "Eating" | purpose == "Recreation (Social Gathering)") %>%  mutate(spend = startingBalance - endingBalance) %>% mutate(date = format(as.Date(checkInTime), "%Y-%m")) %>% mutate(wkday = wday(checkInTime, label = TRUE)) %>% mutate(hr = hour(checkInTime)) 
```

#### Group the sales by the travelEndLocationId, date, weekday and hour

```{r}
sales_hourly <- sales %>% group_by(purpose, travelEndLocationId, date, wkday, hr) %>% summarise(total = sum(spend))
```

#### Pubs data
```{r}
pubs_hourly <- sales_hourly %>% filter(purpose == "Recreation (Social Gathering)")  
```

# 4. Data Visualization

## 4.1. Total sales for pubs

In order to ascertain which pubs are prospering or struggling, we will compute the total sales throughout the priod covered and ranked them. 










